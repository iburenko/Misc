{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN approach for optimizing trading stratergy\n",
    "\n",
    "Macroagent from [arXiv](https://arxiv.org/abs/1812.10252) paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Environment``` class contains the following methods:\n",
    "\n",
    "* ```state_generator```: yields new state;\n",
    "* ```_reward```: calculates reward for specified action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:    \n",
    "    def __init__(self):\n",
    "        self.ticker = \"btcusd\"\n",
    "        self.timeframe = 1\n",
    "        self.train_start_date = datetime(2018, 11, 15)\n",
    "        self.train_end_date = val_start_date = datetime(2018, 11, 16)\n",
    "        self.val_end_date = datetime(2018, 11, 17)\n",
    "        self.ind_lookback = 10\n",
    "        self.volatility_m = 5\n",
    "        self.lookback = 5\n",
    "        self.inds = {'sma': ('sma', self.ind_lookback), 'ema': ('ema', self.ind_lookback)}\n",
    "        self.lots = 10\n",
    "        self.data = self._calculate_states()\n",
    "        self.start_index = self.minute = self.data.dropna().index[0]+self.lookback\n",
    "        self.train_len, self.val_len = self._calculate_lengths()\n",
    "        self.open_price = self.data.open[self.start_index]\n",
    "        self.assets = np.zeros(self.lots)\n",
    "        self.lots_used = 0\n",
    "        \n",
    "        \n",
    "    def _calculate_lengths(self):\n",
    "        train_end_int = int(self.train_end_date.strftime(\"%Y%m%d%H%M%S\"))\n",
    "        train_len = self.data[self.data.time < train_end_int].shape[0] - self.start_index\n",
    "        val_len = self.data[self.data.time >= train_end_int].shape[0]\n",
    "        return train_len, val_len\n",
    "        \n",
    "        \n",
    "    def state_generator(self):\n",
    "        while True:\n",
    "            action = yield\n",
    "            reward = self._reward(action) if action is not None else None\n",
    "            close_prices = self.data['close'][self.minute - self.lookback:self.minute]\n",
    "            ind_vec = self.data[\n",
    "                ['price_z', \n",
    "                'volume_z', \n",
    "                'pc_z', \n",
    "                'vc_z', \n",
    "                'volatility']][self.minute-1:self.minute].stack()\n",
    "            self.minute += 1\n",
    "            self.open_price = self.data.open[self.minute]\n",
    "            yield np.concatenate([ind_vec, close_prices, self.assets]), reward\n",
    "        \n",
    "        \n",
    "    def _calculate_states(self):\n",
    "        def z(series):\n",
    "            mean = series.rolling(self.ind_lookback).sum()/self.ind_lookback\n",
    "            std = series.rolling(self.ind_lookback).std()\n",
    "            return (series-mean)/std\n",
    "        \n",
    "        data = loader.import_candles(\n",
    "            self.ticker, \n",
    "            self.timeframe,\n",
    "            self.train_start_date, \n",
    "            self.val_end_date,\n",
    "            indicators=self.inds, \n",
    "            reverse=False)\n",
    "        data['price_z'] = z(data.close)\n",
    "        data['volume_z'] = z(data.vol)\n",
    "        pc = data.close/(data.close.rolling(self.ind_lookback).sum()/self.ind_lookback) - 1\n",
    "        vc = data.vol/(data.vol.rolling(self.ind_lookback).sum()/self.ind_lookback) - 1\n",
    "        data['pc_z'] = z(pc)\n",
    "        data['vc_z'] = z(vc)\n",
    "        data['volatility'] = 100*(\n",
    "            data.ema - data.ema.shift(self.volatility_m))/data.ema.shift(self.volatility_m)\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def _reward(self, action):\n",
    "        global global_profit, trades\n",
    "        reward_value = None\n",
    "        if action == 1:\n",
    "            reward_value = 0\n",
    "        elif action == 2:\n",
    "            if self.lots_used < self.lots:\n",
    "                self.assets[self.lots_used] = self.open_price\n",
    "                trades += 1\n",
    "                self.lots_used += 1\n",
    "            reward_value = 0\n",
    "        elif action == 0:\n",
    "            if not self.lots_used:\n",
    "                reward_value = -1\n",
    "            else:\n",
    "                pr = np.sum(-self.assets[:self.lots_used]+self.open_price)\n",
    "                global_profit += pr\n",
    "                reward_value = np.sign(np.sum(-self.assets[:self.lots_used]+self.open_price)).astype(np.int8)\n",
    "                self.assets = np.zeros(self.lots)\n",
    "                self.lots_used = 0\n",
    "        return reward_value\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.minute = self.start_index\n",
    "        self.open_price = self.data.open[self.start_index]\n",
    "        self.assets = np.zeros(self.lots)\n",
    "        self.lots_used = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```ReplayMemory``` class contains the following methods:\n",
    "\n",
    "* ```update_M```: to store new SARSA tuple (s, a, r, s', a') in replay memory;\n",
    "* ```batch```: return batch of the specified ```batch_size``` from the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, gen, start_ind):\n",
    "        self.warm_up_length = 60\n",
    "        self.used_memory = 0\n",
    "        self.capacity = 120\n",
    "        self.M = np.zeros((self.capacity, 43))\n",
    "        action = None\n",
    "        next(gen)\n",
    "        state_t, _ = gen.send(action)\n",
    "        for i in range(self.warm_up_length):\n",
    "            action = np.random.randint(0,3)\n",
    "            next(gen)\n",
    "            state_t_next, reward_t = g.send(action)\n",
    "            self.M[i] = np.concatenate(([start_ind+i], state_t, [action, reward_t], state_t_next))\n",
    "            state_t = state_t_next\n",
    "        \n",
    "        \n",
    "    def update_M(self, ind, state_t, action, reward_t, state_t_next):\n",
    "        vec = np.concatenate(([ind], state_t, [action, reward_t], state_t_next))\n",
    "        if self.used_memory >= self.capacity:\n",
    "            self.M = np.roll(self.M, -1, axis=0)\n",
    "            self.M[-1] = vec\n",
    "        else:\n",
    "            self.M[self.used_memory] = vec\n",
    "        self.used_memory += 1\n",
    "        \n",
    "        \n",
    "    def batch(self, batch_size):\n",
    "        upper_lim = np.minimum(self.capacity, np.maximum(self.used_memory, self.warm_up_length))\n",
    "        return self.M[np.random.randint(0, upper_lim, batch_size)]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class ```MacroAgent``` contains the followint methods:\n",
    "\n",
    "* ```update_eps```: to change exploration-exploitation parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacroAgent:    \n",
    "    def __init__(self):\n",
    "        self.eps = 0.99\n",
    "        self.eps_end = 0.1\n",
    "        self.gamma = 0.99\n",
    "        self.epochs = 500\n",
    "        self.eps_step = (self.eps - self.eps_end)/(1400*150)\n",
    "        self.update_target_network = 1000\n",
    "\n",
    "        \n",
    "    def update_eps(self):\n",
    "        if self.eps > self.eps_end:\n",
    "            self.eps -= self.eps_step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for storing profit and the number of trades in Tensorboard\n",
    "class Logger:\n",
    "    def __init__(self, log_dir):\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def log_scalar(self, tag, value, step):\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag,\n",
    "                                                     simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import tradingene.data.load as loader\n",
    "import ipdb\n",
    "from time import time\n",
    "\n",
    "\n",
    "global_profit = trades = 0.\n",
    "\n",
    "\n",
    "### DQN\n",
    "def create_network(net_input):\n",
    "    tf.summary.histogram('input', net_input)\n",
    "    out1 = tf.nn.relu(net_input@weights['layer1']+biases['layer1'])\n",
    "    tf.summary.histogram('out1', out1)\n",
    "    out2 = tf.nn.relu(out1@weights['layer2']+biases['layer2'])\n",
    "    tf.summary.histogram('out2', out2)\n",
    "    out = out2@weights['layer3']+biases['layer3']\n",
    "    tf.summary.histogram('out', out)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Target Network\n",
    "def target_network(target_input):\n",
    "    target_out1 = tf.nn.relu(target_input@target_weights['layer1']+target_biases['layer1'])\n",
    "    target_out2 = tf.nn.relu(target_out1@target_weights['layer2']+target_biases['layer2'])\n",
    "    target_out = target_out2@target_weights['layer3']+target_biases['layer3']\n",
    "    return target_out\n",
    "\n",
    "\n",
    "\n",
    "def calculate_q(batch, gamma, q_vals):\n",
    "    q = np.zeros(batch.shape[0])\n",
    "    q = batch[:,22] + gamma*q_vals\n",
    "    if 1436 in batch[:,0]:\n",
    "        for i in range(batch.shape[0]):\n",
    "            if batch[i, 0] == 1436:\n",
    "                q[i] -= gamma*q_vals[i]   \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialize everything\n",
    "env = Environment()\n",
    "g = env.state_generator()\n",
    "mem = ReplayMemory(g, env.start_index)\n",
    "agent = MacroAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topology = (20, 40, 40, 3)\n",
    "global global_profit, trades\n",
    "batch_size = 20\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Weights and biases for the DQN and the target network:\n",
    "weights = {'layer'+str(i+1):\n",
    "       tf.Variable(tf.random_normal(\n",
    "           [topology[i], topology[i+1]], \n",
    "           mean=.0, \n",
    "           stddev=0.03), name=\"w\"+str(i+1)) \n",
    "       for i in range(len(topology)-1)}\n",
    "biases = {'layer'+str(i+1): \n",
    "      tf.Variable(tf.random_normal(\n",
    "          [topology[i+1]],\n",
    "          mean=.0, \n",
    "          stddev=0.03), name=\"b\"+str(i+1)) \n",
    "      for i in range(len(topology)-1)}\n",
    "target_weights = {'layer'+str(i+1):\n",
    "       tf.Variable(weights['layer'+str(i+1)].initialized_value()) \n",
    "       for i in range(len(topology)-1)}\n",
    "target_biases = {'layer'+str(i+1): \n",
    "      tf.Variable(biases['layer'+str(i+1)].initialized_value()) \n",
    "      for i in range(len(topology)-1)}\n",
    "\n",
    "net_input = tf.placeholder(tf.float32, [None, topology[0]], name=\"net_input\")\n",
    "actions = tf.placeholder(tf.int64, [None], name=\"actions_input\")\n",
    "q = tf.placeholder(tf.float32, [None], name=\"q_input\")\n",
    "Q_values = create_network(net_input)\n",
    "target = tf.reduce_max(target_network(net_input), axis=1)\n",
    "argmax_Q_values = tf.argmax(Q_values, axis=1)\n",
    "action_Q_values = tf.reduce_max(Q_values, axis=1)\n",
    "mask = tf.one_hot(actions, depth=3, dtype=tf.bool, on_value=True, off_value=False)\n",
    "q_pred = tf.boolean_mask(Q_values, mask)\n",
    "error = q - q_pred\n",
    "quadratic_loss = tf.reduce_mean(tf.square(error))\n",
    "loss = tf.reduce_mean(tf.losses.huber_loss(q, q_pred))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(loss)\n",
    "epochs = 2500\n",
    "\n",
    "step = 0\n",
    "init = tf.global_variables_initializer()\n",
    "tf.summary.scalar(\"loss\",loss)\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "fold = 'fold30/1/'\n",
    "fold2 = 'fold30/2/'\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    tr_summary_writer = tf.summary.FileWriter(fold)\n",
    "    logger = Logger(fold2)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"START!\")\n",
    "        t = time()\n",
    "        global_profit = 0.\n",
    "        trades = 0\n",
    "        random_minus_one = random_zero = random_plus_one = 0\n",
    "        minus_one = zero = plus_one = 0\n",
    "        g = env.state_generator()\n",
    "        action = None\n",
    "        next(g)\n",
    "        state_t, _ = g.send(action)\n",
    "        for i in range(env.train_len):\n",
    "            if agent.eps > np.random.uniform():\n",
    "                action_t = np.random.randint(0, 3)\n",
    "                if action_t == 0:\n",
    "                    random_minus_one += 1\n",
    "                elif action_t == 1:\n",
    "                    random_zero += 1\n",
    "                else:\n",
    "                    random_plus_one += 1\n",
    "            else:\n",
    "                action_t_ = sess.run([argmax_Q_values], feed_dict={net_input:np.array([state_t])})\n",
    "                action_t = action_t_[0][0]\n",
    "                if action_t == 2:\n",
    "                    plus_one += 1\n",
    "                elif action_t == 1:\n",
    "                    zero += 1\n",
    "                elif action_t == 0:\n",
    "                    minus_one += 1\n",
    "            next(g)\n",
    "            state_t_next, reward_t = g.send(action_t)\n",
    "            mem.update_M(env.start_index+i, state_t, action_t, reward_t, state_t_next)\n",
    "            batch = mem.batch(batch_size)\n",
    "            Q_values_next = sess.run([target], feed_dict={net_input:batch[:,23:]})\n",
    "            a,b = sess.run([target, action_Q_values], feed_dict={net_input:batch[:,23:]})\n",
    "            q_ = calculate_q(batch, agent.gamma, Q_values_next[0])        \n",
    "            _, loss_value, summ = sess.run([train, loss, merged_summary_op], \n",
    "                                                       feed_dict={net_input:batch[:,1:21], \n",
    "                                                                  actions:batch[:,21],\n",
    "                                                                  q:q_})\n",
    "\n",
    "            agent.update_eps()\n",
    "            state_t = state_t_next\n",
    "            if step % agent.update_target_network == 0:\n",
    "                upd1 = tf.assign(target_weights['layer1'], weights['layer1'].eval())\n",
    "                upd2 = tf.assign(target_biases['layer1'], biases['layer1'].eval())\n",
    "                upd3 = tf.assign(target_weights['layer2'], weights['layer2'].eval())\n",
    "                upd4 = tf.assign(target_biases['layer2'], biases['layer2'].eval())\n",
    "                upd5 = tf.assign(target_weights['layer3'], weights['layer3'].eval())\n",
    "                upd6 = tf.assign(target_biases['layer3'], biases['layer3'].eval())\n",
    "                sess.run([upd1, upd2, upd3, upd4, upd5, upd6])\n",
    "            step += 1\n",
    "        logger.log_scalar(\"profit\", global_profit, epoch)\n",
    "        logger.log_scalar(\"trades\", trades, epoch)\n",
    "        tr_summary_writer.add_summary(summ, epoch)\n",
    "        print(epoch, global_profit, trades, random_minus_one, random_zero, random_plus_one, minus_one, zero, plus_one)\n",
    "        print(time() - t)\n",
    "        if epoch != epochs-1:\n",
    "            env.reset()\n",
    "        print(\"================================================================================\")\n",
    "#         input(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
